{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 16 - Lifecycle Review\n",
    "This notebook is a starting point for polishing your final repo and lifecycle mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist Template\n",
    " - Add checklist elements, as in the examples below, to make sure you cover everything you would like to accomplish\n",
    "- Update this checklist as you finalize your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_clean': True,\n",
       " 'repo_complete': True,\n",
       " 'readme_complete': True,\n",
       " 'lifecycle_map': True,\n",
       " 'summary_doc': True,\n",
       " 'framework_guide_table': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checklist = {\n",
    "    \"repo_clean\": True,\n",
    "    \"repo_complete\": True,\n",
    "    \"readme_complete\": True,\n",
    "    \"lifecycle_map\": True,\n",
    "    \"summary_doc\": True,\n",
    "    \"framework_guide_table\": True\n",
    "}\n",
    "checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Prompts\n",
    "\n",
    "- What stage of the lifecycle was hardest for you, and why?\n",
    "\n",
    "    The hardest stage was **Orchestration & Monitoring**. Coordinating dependencies across ingestion, cleaning, feature engineering, and modeling was complex, and ensuring consistent schemas while tracking failures required careful logging and checkpoints. It highlighted how fragile workflows can be without automation.\n",
    "\n",
    "\n",
    "- Which part of your repo is most reusable in a future project?\n",
    "\n",
    "    The **data ingestion and cleaning modules** (`src/utils.py`, `cleaning.py`, and the structured `data/raw` â†’ `data/processed` pipeline) are the most reusable. They provide a standardized framework for handling raw inputs, applying cleaning functions, and producing consistent processed datasets.\n",
    "\n",
    "- If a teammate had to pick up your repo tomorrow, what would help them most?\n",
    "\n",
    "    A **clear directory structure and documentation** would help them most. The separation of raw vs. processed data, modularized code, and staged notebooks makes it easy to see the lifecycle flow. Explicit README notes, schema consistency, and orchestration diagrams ensure that someone new can quickly trace how inputs become outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
