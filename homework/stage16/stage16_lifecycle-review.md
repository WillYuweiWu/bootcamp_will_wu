# Stage 16 - Lifecycle Review

| Lifecycle Stage | What You Did | Challenges | Solutions / Decisions | Future Improvements |
|-----------------|--------------|------------|-----------------------|---------------------|
| **1. Problem Framing & Scoping** | Defined the financial problem as analyzing VIX & stock market index, with the goal of building clean pipelines, modeling index behavior, and extracting decision insights. | Initial ambiguity on scope (e.g. how long to trace back the indices). | Chose the most popular stock index in the world, i.e. S&P 500 as the target and trace the data for 6 years. | Spend more time upfront on success metrics (e.g., model accuracy targets, business-driven KPIs). |
| **2. Tooling Setup** | Configured Python environment and GitHub project structure. | Package install issues (`yfinance` not found, Alpha Vantage API setup), `.env` misconfiguration, parquet vs. csv handling. | Fixed by managing dependencies in `requirements.txt`, centralizing keys in `.env`, debugging file paths. | Automate setup with `Makefile` or Docker; CI/CD pipeline to check environment consistency. |
| **3. Python Fundamentals** | Applied core Python/data skills: file I/O, dataframes, loops, list comprehensions, plotting, modular coding (`utils.py`). | Syntax mistakes and library usage gaps (e.g., seaborn vs. matplotlib, string f-strings in plots). | Iterated through errors, simplified functions, practiced modularization. | Continue strengthening debugging fluency and writing more pythonic, vectorized code. |
| **4. Data Acquisition / Ingestion** | Acquired data via CSV, APIs (yfinance/Alpha Vantage), and synthetic generation (options across tickers, industries, time windows). | Inconsistent availability of market data, API limits, and CSV directory pathing issues. | Created repeatable ingestion scripts, synthetic dataset with realistic structure (date, VIX, S&P 500). | Improve robustness by adding schema validation, logging, and retry logic for APIs. |
| **5. Data Storage** | Stored raw data in `/data/raw`, processed outputs in `/data/processed` using CSV/Parquet. | Confusion around CSV vs. Parquet (speed vs. compatibility), schema drift across stages. | Standardized schema and column naming conventions; chose CSV for portability, Parquet for efficiency. | Introduce a database (SQLite/Postgres) for larger projects; enforce schema with `pydantic` or `great_expectations`. |
| **6. Data Preprocessing** | Ran cleaning steps: drop NAs vs. threshold, normalization, filtering invalid rows, moving utility functions to `cleaning.py`. | Errors when mixing `dropna` args; duplicate handling logic. | Modularized cleaning functions (drop_na, drop_threshold, standardize). | Automate profiling reports to quickly spot anomalies. |
| **7. Outlier Analysis** | Detected anomalies in both indices (e.g. extreme spreads). | Deciding between noise vs. valid financial shocks. | Kept realistic financial shocks (e.g., volatility spikes), dropped clear data entry errors. | Use statistical tests or rolling-window thresholds to automate outlier treatment. |
| **8. Exploratory Data Analysis (EDA)** | Produced histograms, distributions, scatter plots, rolling stats; summarized features like bid-ask spread, ITM indicator. | Some plots initially misleading (scale issues, too few data points). | Iterated with better labeling, industry-specific cuts. | Add interactive dashboards (Plotly, Dash) for dynamic exploration. |
| **9. Feature Engineering** | Built features: DTE (days-to-expiry), ITM flag, bid-ask spread, rolling stats, ratios. | Justifying feature value (financial relevance vs. noise). | Validated via domain reasoning (spread shows liquidity, ITM affects moneyness). | Add technical indicators, implied volatility surfaces, regime markers. |
| **10. Modeling (Regression / Time Series / Classification)** | Applied regression models to option pricing data; evaluated R², RMSE; simulated Brownian motion for option pricing paths. | Challenges with convergence, overfitting, and runtime with large paths. | Tuned path counts (10k vs. 20k), explained baseline vs. improved metrics. | Try more advanced models (tree ensembles, GARCH, stochastic volatility). |
| **11. Evaluation & Risk Communication** | Evaluated models using R², RMSE; communicated risk around assumptions (e.g., stationarity, synthetic data limitations). | Hard to convey uncertainty in outputs to non-technical stakeholders. | Used clear visuals, simple summaries (“baseline vs. improved R²”). | Add confidence intervals, stress-testing scenarios. |
| **12. Results Reporting, Delivery Design & Stakeholder Communication** | Delivered via Jupyter notebooks, markdown write-ups, and slide summaries. | Translating technical regression/option results to business implications. | Structured outputs into sections (Takeaways, Decision Implications). | Build more polished dashboards or PDF reports for decision-makers. |
| **13. Productization** | Designed pipelines to be reusable across datasets; separated raw vs. processed vs. src utilities. | Maintainability issues as project grew (multiple scripts, repeated logic). | Standardized file structure, modularized code. | Containerize with Docker, package into reusable library. |
| **14. Deployment & Monitoring** | Simulated monitoring by setting metrics/alerts (schema drift, PSI thresholds, rolling AUC checks). | No live deployment, so limited real monitoring. | Documented failure modes, monitoring plan (Data, Model, System, Business layers). | Implement in production with Airflow + monitoring dashboards. |
| **15. Orchestration & System Design** | Mapped DAG dependencies (data ingest → clean → feature eng → model → report). | Complex dependencies across tasks, checkpoints/logging. | Created orchestration plan with checkpoints, logging, manual vs. auto trade-offs. | Automate full DAG with Prefect/Airflow; add retry/failure recovery. |
| **16. Lifecycle Review & Reflection** | Biggest takeaway: full lifecycle discipline (from data acquisition to reporting) is crucial, not just modeling. | Struggled most with orchestration & monitoring — keeping everything consistent across stages. | Strategies that helped: modular file structure, iterative refinement, structured stage deliverables. | Next time: clarify metrics earlier, automate setup/ingestion, add stronger monitoring, emphasize communication skills. |

## Reflection Prompts

- **Most difficult stage**: Orchestration & Monitoring — coordinating dependencies, logging, and ensuring consistent schemas was the hardest.  
- **Most rewarding stage**: Exploratory Data Analysis — visualizing patterns and building domain-driven features that illuminated the dataset.  
- **Connections across stages**: Early schema/storage decisions constrained modeling; strong preprocessing and feature design enabled clearer evaluation.  
- **If repeated**: Clarify scope earlier, enforce schema contracts, automate setup, and push toward production-grade orchestration/monitoring.  
- **Skills to strengthen**: Advanced feature engineering, communication of uncertainty, production deployment tools (Airflow, Docker), and financial domain modeling (stochastic volatility, implied vol surfaces).  